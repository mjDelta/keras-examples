{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the raw IMDB movie data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "imdb_dir=\"aclImdb\"\n",
    "train_dir=os.path.join(imdb_dir,\"train\")\n",
    "\n",
    "labels=[]\n",
    "texts=[]\n",
    "for label_type in [\"neg\",\"pos\"]:\n",
    "    dir_name=os.path.join(train_dir,label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:]==\".txt\":\n",
    "            f=open(os.path.join(dir_name,fname),encoding=\"UTF-8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type==\"neg\":\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "print(labels[0])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen=20\n",
    "max_words=10000\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "labels=np.asarray(labels)\n",
    "indexs=np.arange(data.shape[0])\n",
    "np.random.shuffle(indexs)\n",
    "x_train=data[indexs];y_train=labels[indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 words. \n"
     ]
    }
   ],
   "source": [
    "glove_dir=\"glove.6B.100d\"\n",
    "embedding_index={}\n",
    "f=open(os.path.join(glove_dir,\"glove.6B.100d.txt\"),encoding=\"UTF-8\")\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype=\"float32\")##turn string into float32\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "print(\"Found %s words. \"%len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "word_index=tokenizer.word_index\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if i<max_words and embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 1,002,001\n",
      "Trainable params: 1,002,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Flatten,Dense\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.6527 - acc: 0.6163 - val_loss: 0.6250 - val_acc: 0.6452\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5956 - acc: 0.6784 - val_loss: 0.5985 - val_acc: 0.6732\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.5784 - acc: 0.6938 - val_loss: 0.6046 - val_acc: 0.6754\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 125us/step - loss: 0.5674 - acc: 0.7058 - val_loss: 0.6095 - val_acc: 0.6702\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 126us/step - loss: 0.5611 - acc: 0.7096 - val_loss: 0.6450 - val_acc: 0.6572\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.5570 - acc: 0.7153 - val_loss: 0.6225 - val_acc: 0.6648\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 135us/step - loss: 0.5537 - acc: 0.7154 - val_loss: 0.5973 - val_acc: 0.6898\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 134us/step - loss: 0.5509 - acc: 0.7204 - val_loss: 0.6023 - val_acc: 0.6838\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 137us/step - loss: 0.5469 - acc: 0.7202 - val_loss: 0.6037 - val_acc: 0.6784\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.5477 - acc: 0.7215 - val_loss: 0.6054 - val_acc: 0.6836\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
